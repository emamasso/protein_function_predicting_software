{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4775bb0a-e0de-4df0-97d4-d9a1b4d018fe",
   "metadata": {},
   "source": [
    "### In this script you can find the commands needed to import and combine all the data.\n",
    "### First all the files are imported, then combined in all the data structures needed: one with the features and one with the output to predict, for each sub-ontology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d305f0-6760-4160-a2e7-c0de93b69c7e",
   "metadata": {},
   "source": [
    "#### Necessary libraries for the data loading/ preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b7512c-c10d-498c-81ea-9fbb7ce00bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from Bio import SeqIO\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a311c1-d1b3-4d03-90b9-ffb8725a33eb",
   "metadata": {},
   "source": [
    "#### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c6f7aab-318f-464e-868b-c230f229fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the train_set dataset\n",
    "df = pd.read_csv('data/train/train_set.tsv', sep = '\\t') ## insert your path to the file\n",
    "# split it into the sub-ontologies\n",
    "mf_df = df[df['aspect'] == 'molecular_function']\n",
    "cc_df = df[df['aspect'] == 'cellular_component']\n",
    "bp_df = df[df['aspect'] == 'biological_process']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7301da1f-f3cc-441c-8234-fd825c0f0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary to filter as the professor already filtered the subontologies\n",
    "# and proteins with sequence length of 2000+ also have been removed\n",
    "count_mf = mf_df['GO_term'].value_counts() \n",
    "mf_keep = count_mf[count_mf >= 50].index.tolist()\n",
    "y_mf = mf_df[mf_df['GO_term'].isin(mf_keep)]\n",
    "\n",
    "count_cc = cc_df['GO_term'].value_counts() \n",
    "cc_keep = count_cc[count_cc >= 50].index.tolist()\n",
    "y_cc = cc_df[cc_df['GO_term'].isin(cc_keep)]\n",
    "\n",
    "count_bp = bp_df['GO_term'].value_counts() \n",
    "bp_keep = count_bp[count_bp >= 250].index.tolist()\n",
    "y_bp = bp_df[bp_df['GO_term'].isin(bp_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f9fb590-d30a-457a-837e-8c2657148c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing train_ids\n",
    "with open('data/train/train_ids.txt') as f: ##insert path to the file\n",
    "    train_ids = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "130f6b43-12bf-4d72-b2cf-c1a8c07e91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of domains and proteins\n",
    "id_map = {pid: i for i, pid in enumerate(train_ids)}  ## this is quicker than the original code\n",
    "#in this dictionary we have a set of all the proteins and their positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65194914-337e-4b92-a9a0-c57df5298205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing train_protein2ipr.dat, as protein_to_ipr\n",
    "column_names = ['Protein_ID', 'InterPro_ID', 'Domain_Name', 'Source_DB', 'Start', 'End']\n",
    "\n",
    "protein_to_ipr = pd.read_csv(\"data/train/train_protein2ipr.dat\",  ## insert your path to the file\n",
    "                            sep='\\t', names = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ce73995-80ba-4a36-9ce5-5aecc9eb5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there are thousands of domains we could keep the 1000 most represented\n",
    "# (about the 70% of the total proteins are in these) so our pc doesn't explode during training\n",
    "\n",
    "domain_count = protein_to_ipr[\"InterPro_ID\"].value_counts()\n",
    "top_1000 = domain_count.iloc[:1000].index.tolist()\n",
    "\n",
    "# filter only rows with selected domains\n",
    "protein_to_ipr_filtered = protein_to_ipr[protein_to_ipr[\"InterPro_ID\"].isin(top_1000)]\n",
    "\n",
    "\n",
    "#################### might present a problem for MF prediction since\n",
    "#################### some functions are rare and require specific domains\n",
    "\n",
    "# map domain → column index\n",
    "ipr_map = {ipr: i for i, ipr in enumerate(top_1000)} #this is quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60c7a92a-c332-498b-86c3-514ea714dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on this code - 1000 top domains are most representative, with reasonable dimensionality reduction\n",
    "# protein_to_ipr: full mapping df with Protein_ID, InterPro_ID\n",
    "# domain_counts = protein_to_ipr[\"InterPro_ID\"].value_counts()\n",
    "\n",
    "# def coverage_at_k(k):\n",
    "   # topk = set(domain_counts.head(k).index)\n",
    "   # covered = protein_to_ipr[protein_to_ipr[\"InterPro_ID\"].isin(topk)][\"Protein_ID\"].nunique()\n",
    "   # total = protein_to_ipr[\"Protein_ID\"].nunique()\n",
    "   # return covered / total\n",
    "\n",
    "# for k in [200, 500, 1000, 2000, 5000]:\n",
    "   # print(k, round(coverage_at_k(k)*100, 2), \"% proteins covered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bd0cb8f-61e5-41a3-9477-aa30518e669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mhem = np.zeros((len(train_ids), len(top_1000)), dtype=np.int8)\n",
    "\n",
    "for row in protein_to_ipr_filtered.itertuples():\n",
    "    pid = row.Protein_ID\n",
    "    ipr = row.InterPro_ID\n",
    "    if pid in id_map:\n",
    "        row_i = id_map[pid]\n",
    "        col_i = ipr_map[ipr]\n",
    "        mhem[row_i, col_i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "961ef4a4-e24b-4f26-b72e-830bb9f2c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ordered_embedding_matrix(h5_path, ordered_ids):\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        # detect embedding dimension\n",
    "        sample_pid = ordered_ids[0]\n",
    "        sample = f[sample_pid][()]\n",
    "        emb_dim = sample.shape[-1]\n",
    "\n",
    "        X = np.zeros((len(ordered_ids), emb_dim), dtype=np.float32)\n",
    "\n",
    "        for i, pid in enumerate(ordered_ids):\n",
    "            if pid in f:\n",
    "                vec = f[pid][()]\n",
    "                # mean pool if residue-level embeddings\n",
    "                if vec.ndim == 2:\n",
    "                    vec = vec.mean(axis=0)\n",
    "                X[i] = vec\n",
    "        return X\n",
    "\n",
    "emb_matrix = get_ordered_embedding_matrix(\n",
    "    \"data/train/train_embeddings.h5\",\n",
    "    train_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cc9b463-7593-4ea3-b27a-f3eb53518c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((mhem.astype(np.float32), emb_matrix), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e38572e-df5d-453e-ac9b-ff046dfb9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_fun(go_list):\n",
    "    return {go_term: i for i, go_term in enumerate(go_list)}\n",
    "\n",
    "go_mf_map = mapping_fun(mf_keep)\n",
    "go_cc_map = mapping_fun(cc_keep)\n",
    "go_bp_map = mapping_fun(bp_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e435ad7-01e2-48ad-bfb7-632489557ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mf_train = np.zeros((len(train_ids), len(mf_keep)), dtype=np.int8)\n",
    "for row in y_mf.itertuples():\n",
    "    pid = row.Protein_ID\n",
    "    go = row.GO_term\n",
    "    if pid in id_map and go in go_mf_map:\n",
    "        row_i = id_map[pid]\n",
    "        col_j = go_mf_map[go]\n",
    "        Y_mf_train[row_i, col_j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b49026e6-962b-4e31-80b4-a489293b179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_cc_train = np.zeros((len(train_ids), len(cc_keep)), dtype=np.int8)\n",
    "for row in y_cc.itertuples():\n",
    "    pid = row.Protein_ID\n",
    "    go = row.GO_term\n",
    "    if pid in id_map and go in go_cc_map:\n",
    "        row_i = id_map[pid]\n",
    "        col_j = go_cc_map[go]\n",
    "        Y_cc_train[row_i, col_j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7515271-44a6-4c46-acbb-85cf38976ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_bp_train = np.zeros((len(train_ids), len(bp_keep)), dtype=np.int8)\n",
    "for row in y_bp.itertuples():\n",
    "    pid = row.Protein_ID\n",
    "    go = row.GO_term\n",
    "    if pid in id_map and go in go_bp_map:\n",
    "        row_i = id_map[pid]\n",
    "        col_j = go_bp_map[go]\n",
    "        Y_bp_train[row_i, col_j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b521c8b3-ebf6-43fc-9946-6f38bfa2d6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n",
      "X_train shape: (123969, 2024)\n",
      "Y_cc shape: (123969, 677)\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"data/train/final_data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(output_dir, \"X_train.npy\"), X_train)\n",
    "np.save(os.path.join(output_dir, \"Y_mf_train.npy\"), Y_mf_train)\n",
    "np.save(os.path.join(output_dir, \"Y_cc_train.npy\"), Y_cc_train)\n",
    "np.save(os.path.join(output_dir, \"Y_bp_train.npy\"), Y_bp_train)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_cc shape:\", Y_cc_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52412723-fe72-4b20-9a39-e8ad77864fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CC positives: 1109583\n"
     ]
    }
   ],
   "source": [
    "print(\"Total CC positives:\", Y_cc_train.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3299a4a5-3381-4fd5-aa47-27f8057902a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/test/test_ids.txt\") as f:\n",
    "    test_ids = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7b4f44-7fbf-42ca-abcd-bebb101f2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Protein_ID', 'InterPro_ID', 'Domain_Name', 'Source_DB', 'Start', 'End']\n",
    "\n",
    "test_ipr = pd.read_csv(\n",
    "    \"data/test/test_protein2ipr.dat\",\n",
    "    sep=\"\\t\",\n",
    "    names=column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edfcd1ab-42a4-4581-a9e2-84ccb4623138",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ipr_filtered = test_ipr[test_ipr[\"InterPro_ID\"].isin(top_1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc900727-8ac1-4c51-a0e1-8cc6d7490442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map for test IDs\n",
    "test_id_map = {pid: i for i, pid in enumerate(test_ids)}\n",
    "\n",
    "# Create test multi-hot encoding matrix\n",
    "mhem_test = np.zeros((len(test_ids), len(top_1000)), dtype=np.int8)\n",
    "\n",
    "for row in test_ipr_filtered.itertuples():\n",
    "    pid = row.Protein_ID\n",
    "    ipr = row.InterPro_ID\n",
    "    if pid in test_id_map and ipr in ipr_map:\n",
    "        mhem_test[test_id_map[pid], ipr_map[ipr]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "136ae31f-13c4-46a4-b3c3-5e3d2e49b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_test = get_ordered_embedding_matrix(\n",
    "    \"data/test/test_embeddings.h5\",\n",
    "    test_ids\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37f68ece-7155-4d0e-a7c4-eee57dd55481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (1000, 2024)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.concatenate((mhem_test, emb_test), axis=1)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27132425-d34e-470a-9e4a-23a60e2bfd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/test/X_test.npy\", X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5caaf483-75e1-47ba-9892-12ef77dce891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All metadata saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "SAVE_PATH = \"saved_cc_metadata\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# 1. Save GO terms for CC\n",
    "np.save(f\"{SAVE_PATH}/cc_keep.npy\", np.array(cc_keep, dtype=object))\n",
    "\n",
    "# 2. Save the GO mapping dict\n",
    "with open(f\"{SAVE_PATH}/go_cc_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(go_cc_map, f)\n",
    "\n",
    "# 3. Save training protein ID list\n",
    "np.save(f\"{SAVE_PATH}/train_ids.npy\", np.array(train_ids, dtype=object))\n",
    "\n",
    "# 4. Save id_map (protein → row index in training matrices)\n",
    "with open(f\"{SAVE_PATH}/id_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id_map, f)\n",
    "\n",
    "# 5. Save top 1000 domains\n",
    "np.save(f\"{SAVE_PATH}/top_1000.npy\", np.array(top_1000, dtype=object))\n",
    "\n",
    "# 6. Save InterPro domain mapping dict\n",
    "with open(f\"{SAVE_PATH}/ipr_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ipr_map, f)\n",
    "\n",
    "print(\"All metadata saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68069252-4938-428e-96b7-4ed8cdb8374a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All metadata saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "SAVE_PATH = \"saved_mf_metadata\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# 1. Save GO terms for CC\n",
    "np.save(f\"{SAVE_PATH}/mf_keep.npy\", np.array(mf_keep, dtype=object))\n",
    "\n",
    "# 2. Save the GO mapping dict\n",
    "with open(f\"{SAVE_PATH}/go_mf_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(go_mf_map, f)\n",
    "\n",
    "# 3. Save training protein ID list\n",
    "np.save(f\"{SAVE_PATH}/train_ids.npy\", np.array(train_ids, dtype=object))\n",
    "\n",
    "# 4. Save id_map (protein → row index in training matrices)\n",
    "with open(f\"{SAVE_PATH}/id_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id_map, f)\n",
    "\n",
    "# 5. Save top 1000 domains\n",
    "np.save(f\"{SAVE_PATH}/top_1000.npy\", np.array(top_1000, dtype=object))\n",
    "\n",
    "# 6. Save InterPro domain mapping dict\n",
    "with open(f\"{SAVE_PATH}/ipr_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ipr_map, f)\n",
    "\n",
    "print(\"All metadata saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
